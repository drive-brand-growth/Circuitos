#!/usr/bin/env python3
"""
Circuit OS Adaptive RAG - Production-Grade Components
Query Router, Semantic Cache, and Quality Gates (2025 SOTA)
"""

import os
import re
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import redis
from openai import OpenAI

logger = logging.getLogger(__name__)


# =============================================================================
# QUERY ROUTER (3-Tier Adaptive Routing)
# =============================================================================

class QueryRouter:
    """
    Route queries to appropriate retrieval strategy based on complexity.

    Tiers:
    - NO_RETRIEVAL: LLM can answer directly (saves embedding + search cost)
    - SINGLE_STEP: Standard RAG retrieval (most queries)
    - MULTI_STEP: Complex reasoning requiring iterative retrieval
    """

    def __init__(self, openai_api_key: str = None):
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        self._client = None

        # Simple heuristic patterns
        self.no_retrieval_patterns = [
            r'hello|hi|hey|greetings',
            r'thank you|thanks',
            r'goodbye|bye',
            r'what is your name',
            r'who are you',
            r'how are you',
        ]

        self.multi_step_patterns = [
            r'compare .* and .*',
            r'difference between .* and .*',
            r'analyze .* across .*',
            r'all .* events',
            r'summarize .* from .*',
        ]

    def _get_client(self):
        """Lazy OpenAI client initialization"""
        if self._client is None and self.openai_api_key:
            self._client = OpenAI(api_key=self.openai_api_key)
        return self._client

    def classify_query(self, query: str) -> Dict[str, Any]:
        """
        Classify query complexity and route appropriately.

        Returns:
            {
                'route': 'no_retrieval' | 'single_step' | 'multi_step',
                'reasoning': str,
                'confidence': float
            }
        """
        query_lower = query.lower()

        # Fast heuristic checks first
        for pattern in self.no_retrieval_patterns:
            if re.search(pattern, query_lower):
                return {
                    'route': 'no_retrieval',
                    'reasoning': 'Simple greeting/social query - LLM can answer directly',
                    'confidence': 0.95
                }

        for pattern in self.multi_step_patterns:
            if re.search(pattern, query_lower):
                return {
                    'route': 'multi_step',
                    'reasoning': 'Complex comparison/analysis query requiring multiple retrievals',
                    'confidence': 0.85
                }

        # Length-based heuristics
        word_count = len(query.split())
        if word_count <= 5 and '?' not in query:
            return {
                'route': 'no_retrieval',
                'reasoning': 'Very short query without question mark',
                'confidence': 0.75
            }

        # Default to single-step for most queries
        if word_count <= 20:
            return {
                'route': 'single_step',
                'reasoning': 'Standard factual query suitable for single RAG retrieval',
                'confidence': 0.80
            }

        # Long, complex queries
        return {
            'route': 'multi_step',
            'reasoning': 'Long query may require multiple retrieval steps',
            'confidence': 0.70
        }

    def should_retrieve(self, query: str) -> bool:
        """Quick check if retrieval is needed"""
        route = self.classify_query(query)
        return route['route'] != 'no_retrieval'


# =============================================================================
# SEMANTIC CACHE (Redis-based)
# =============================================================================

class SemanticCache:
    """
    Redis-backed semantic cache for RAG queries.
    Reduces redundant LLM calls by ~31% for similar queries.
    """

    def __init__(self, redis_url: str, embedder, ttl: int = 3600):
        """
        Args:
            redis_url: Redis connection URL
            embedder: Embedder instance for query encoding
            ttl: Cache TTL in seconds (default 1 hour)
        """
        self.redis_client = redis.from_url(redis_url, decode_responses=False)
        self.embedder = embedder
        self.ttl = ttl
        self.similarity_threshold = 0.95  # High threshold for cache hits

        logger.info(f"Semantic cache initialized (TTL: {ttl}s, threshold: {self.similarity_threshold})")

    def _generate_cache_key(self, query_embedding: List[float]) -> str:
        """Generate cache key from query embedding"""
        # Use first 8 dimensions for bucketing (approximate nearest neighbor)
        bucket = hashlib.md5(
            json.dumps(query_embedding[:8]).encode()
        ).hexdigest()[:8]
        return f"rag:cache:{bucket}"

    def get(self, query: str) -> Optional[Dict]:
        """
        Check cache for semantically similar query.

        Returns cached response if similarity >= threshold, else None.
        """
        try:
            # Embed the query
            query_embedding = self.embedder.embed_text(query)

            # Get cache bucket
            cache_key = self._generate_cache_key(query_embedding)

            # Retrieve all entries in bucket
            cached_entries = self.redis_client.hgetall(cache_key)

            if not cached_entries:
                return None

            # Find best match
            best_match = None
            best_similarity = 0.0

            for cached_query_hash, cached_data_json in cached_entries.items():
                cached_data = json.loads(cached_data_json)
                cached_embedding = cached_data['query_embedding']

                # Compute cosine similarity
                similarity = self._cosine_similarity(query_embedding, cached_embedding)

                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = cached_data

            if best_match:
                logger.info(f"âœ… Cache HIT (similarity: {best_similarity:.3f})")
                return {
                    'response': best_match['response'],
                    'results': best_match['results'],
                    'cached': True,
                    'similarity': best_similarity,
                    'original_query': best_match['query']
                }

            logger.info("âŒ Cache MISS")
            return None

        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None

    def set(self, query: str, response: str, results: List[Dict]):
        """
        Store query-response pair in cache.
        """
        try:
            # Embed the query
            query_embedding = self.embedder.embed_text(query)

            # Generate cache key
            cache_key = self._generate_cache_key(query_embedding)
            query_hash = hashlib.md5(query.encode()).hexdigest()

            # Prepare cache entry
            cache_entry = {
                'query': query,
                'query_embedding': query_embedding,
                'response': response,
                'results': results,
                'timestamp': datetime.utcnow().isoformat()
            }

            # Store in Redis hash with TTL
            self.redis_client.hset(cache_key, query_hash, json.dumps(cache_entry))
            self.redis_client.expire(cache_key, self.ttl)

            logger.info(f"ðŸ’¾ Cached query: {query[:50]}...")

        except Exception as e:
            logger.error(f"Cache set error: {e}")

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Compute cosine similarity between two vectors"""
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def clear(self):
        """Clear all cache entries"""
        keys = self.redis_client.keys("rag:cache:*")
        if keys:
            self.redis_client.delete(*keys)
            logger.info(f"ðŸ—‘ï¸  Cleared {len(keys)} cache buckets")

    def stats(self) -> Dict:
        """Get cache statistics"""
        keys = self.redis_client.keys("rag:cache:*")
        total_entries = sum(self.redis_client.hlen(key) for key in keys)

        return {
            'buckets': len(keys),
            'total_entries': total_entries,
            'ttl': self.ttl,
            'threshold': self.similarity_threshold
        }


# =============================================================================
# QUALITY GATES (Hallucination Detection & Self-Correction)
# =============================================================================

class QualityGate:
    """
    Multi-stage quality assurance for RAG responses.

    Gates:
    1. Document Relevance Assessment
    2. Hallucination Detection
    3. Answer Quality Evaluation
    """

    def __init__(self, openai_api_key: str = None):
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        self._client = None
        self.model = "gpt-4o-mini"

    def _get_client(self):
        """Lazy OpenAI client initialization"""
        if self._client is None and self.openai_api_key:
            self._client = OpenAI(api_key=self.openai_api_key)
        return self._client

    def check_document_relevance(self, query: str, documents: List[str]) -> Dict:
        """
        Grade if retrieved documents are relevant to the query.

        Returns:
            {
                'relevant': bool,
                'score': float (0-1),
                'reasoning': str
            }
        """
        if not documents:
            return {'relevant': False, 'score': 0.0, 'reasoning': 'No documents retrieved'}

        try:
            client = self._get_client()
            if not client:
                return {'relevant': True, 'score': 0.5, 'reasoning': 'No LLM available for grading'}

            docs_text = "\n\n".join([f"Document {i+1}: {doc[:200]}..." for i, doc in enumerate(documents)])

            prompt = f"""You are a document relevance grader.

Query: {query}

Retrieved Documents:
{docs_text}

Are these documents relevant to answering the query? Respond in JSON format:
{{
  "relevant": true/false,
  "score": 0.0-1.0,
  "reasoning": "brief explanation"
}}"""

            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=150
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"Relevance check error: {e}")
            return {'relevant': True, 'score': 0.5, 'reasoning': f'Error: {e}'}

    def detect_hallucination(self, query: str, answer: str, documents: List[str]) -> Dict:
        """
        Check if answer is grounded in retrieved documents.

        Returns:
            {
                'hallucinated': bool,
                'confidence': float (0-1),
                'issues': List[str]
            }
        """
        if not documents:
            return {'hallucinated': True, 'confidence': 1.0, 'issues': ['No source documents']}

        try:
            client = self._get_client()
            if not client:
                return {'hallucinated': False, 'confidence': 0.5, 'issues': []}

            docs_text = "\n\n".join([f"[Source {i+1}]: {doc}" for i, doc in enumerate(documents)])

            prompt = f"""You are a hallucination detector.

Query: {query}

Answer: {answer}

Source Documents:
{docs_text}

Is the answer fully supported by the source documents? Check for:
1. Claims not present in sources
2. Contradictions with sources
3. Invented details or statistics

Respond in JSON format:
{{
  "hallucinated": true/false,
  "confidence": 0.0-1.0,
  "issues": ["list of specific hallucinations if any"]
}}"""

            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=200
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"Hallucination detection error: {e}")
            return {'hallucinated': False, 'confidence': 0.5, 'issues': []}

    def evaluate_answer_quality(self, query: str, answer: str) -> Dict:
        """
        Evaluate overall answer quality.

        Returns:
            {
                'quality_score': float (0-1),
                'completeness': float (0-1),
                'clarity': float (0-1),
                'needs_improvement': bool
            }
        """
        try:
            client = self._get_client()
            if not client:
                return {'quality_score': 0.7, 'completeness': 0.7, 'clarity': 0.7, 'needs_improvement': False}

            prompt = f"""You are an answer quality evaluator.

Query: {query}
Answer: {answer}

Evaluate the answer on:
1. Completeness: Does it fully address the query?
2. Clarity: Is it clear and well-structured?

Respond in JSON format:
{{
  "quality_score": 0.0-1.0,
  "completeness": 0.0-1.0,
  "clarity": 0.0-1.0,
  "needs_improvement": true/false
}}"""

            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=150
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"Quality evaluation error: {e}")
            return {'quality_score': 0.7, 'completeness': 0.7, 'clarity': 0.7, 'needs_improvement': False}

    def validate_response(self, query: str, answer: str, documents: List[str]) -> Dict:
        """
        Run all quality gates and provide comprehensive validation.

        Returns:
            {
                'passed': bool,
                'relevance': dict,
                'hallucination': dict,
                'quality': dict,
                'recommendation': str
            }
        """
        # Gate 1: Document relevance
        relevance = self.check_document_relevance(query, documents)

        # Gate 2: Hallucination detection
        hallucination = self.detect_hallucination(query, answer, documents)

        # Gate 3: Answer quality
        quality = self.evaluate_answer_quality(query, answer)

        # Determine if response passed all gates
        passed = (
            relevance.get('relevant', False) and
            not hallucination.get('hallucinated', False) and
            not quality.get('needs_improvement', True)
        )

        # Generate recommendation
        if not relevance.get('relevant', False):
            recommendation = 'RETRIEVE_MORE'  # Documents not relevant, try web search
        elif hallucination.get('hallucinated', False):
            recommendation = 'REGENERATE'  # Answer has hallucinations, regenerate
        elif quality.get('needs_improvement', True):
            recommendation = 'IMPROVE'  # Answer quality low, improve
        else:
            recommendation = 'APPROVED'

        return {
            'passed': passed,
            'relevance': relevance,
            'hallucination': hallucination,
            'quality': quality,
            'recommendation': recommendation
        }


# =============================================================================
# FEEDBACK-DRIVEN RERANKER
# =============================================================================

class FeedbackReranker:
    """
    Rerank search results based on user feedback.
    Boosts chunks with positive feedback, penalizes negative.
    """

    def __init__(self, vector_store):
        self.vector_store = vector_store
        logger.info("Feedback reranker initialized")

    def apply_feedback_weights(self, results: List[Dict]) -> List[Dict]:
        """
        Apply feedback-based weights to search results.

        Returns reranked results with adjusted similarity scores.
        """
        try:
            with self.vector_store.conn.cursor() as cur:
                # Get feedback scores for all chunks
                chunk_ids = [r['chunk_id'] for r in results]

                if not chunk_ids:
                    return results

                placeholders = ','.join(['%s'] * len(chunk_ids))
                cur.execute(f"""
                    SELECT chunk_id, AVG(feedback_value) as avg_feedback, COUNT(*) as feedback_count
                    FROM rag_feedback
                    WHERE chunk_id IN ({placeholders})
                    GROUP BY chunk_id;
                """, chunk_ids)

                feedback_map = {row[0]: {'avg': float(row[1]), 'count': row[2]} for row in cur.fetchall()}

                # Apply feedback weights
                for result in results:
                    chunk_id = result['chunk_id']
                    base_similarity = result['similarity']

                    if chunk_id in feedback_map:
                        fb = feedback_map[chunk_id]
                        # Boost/penalize based on feedback average
                        # More feedback = stronger impact
                        weight = 1.0 + (fb['avg'] * min(fb['count'] / 10.0, 0.3))  # Max 30% adjustment
                        result['similarity'] = min(base_similarity * weight, 1.0)
                        result['feedback_adjusted'] = True
                        result['feedback_avg'] = fb['avg']
                        result['feedback_count'] = fb['count']
                    else:
                        result['feedback_adjusted'] = False

                # Re-sort by adjusted similarity
                results.sort(key=lambda x: x['similarity'], reverse=True)

                logger.info(f"Reranked {len(results)} results with feedback")
                return results

        except Exception as e:
            logger.error(f"Feedback reranking error: {e}")
            return results
