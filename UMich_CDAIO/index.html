<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>UMich CDAIO - Mobile Audio Training</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            -webkit-tap-highlight-color: transparent;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #00274C 0%, #001A33 100%);
            color: #FFFFFF;
            line-height: 1.6;
            min-height: 100vh;
            padding-bottom: 100px;
            overflow-x: hidden;
        }

        .container {
            max-width: 100%;
            padding: 15px;
        }

        header {
            text-align: center;
            padding: 25px 15px;
            background: linear-gradient(135deg, rgba(255, 203, 5, 0.1) 0%, rgba(255, 203, 5, 0.05) 100%);
            border-radius: 20px;
            border: 2px solid rgba(255, 203, 5, 0.3);
            margin-bottom: 25px;
        }

        .logo {
            width: 80px;
            height: 80px;
            margin: 0 auto 15px;
            background: linear-gradient(135deg, #00274C 0%, #FFCB05 100%);
            border-radius: 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2em;
            font-weight: 700;
        }

        h1 {
            font-size: 1.8em;
            font-weight: 700;
            margin-bottom: 8px;
            background: linear-gradient(135deg, #FFCB05 0%, #FFFFFF 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            font-size: 1em;
            color: #B0B8C0;
            margin-bottom: 15px;
        }

        .stats {
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
            margin-top: 15px;
        }

        .stat {
            text-align: center;
            padding: 10px 15px;
            background: rgba(255, 203, 5, 0.1);
            border-radius: 10px;
        }

        .stat-number {
            font-size: 1.5em;
            font-weight: 700;
            color: #FFCB05;
        }

        .stat-label {
            font-size: 0.8em;
            color: #B0B8C0;
        }

        .progress-overview {
            background: rgba(20, 21, 23, 0.6);
            border: 2px solid rgba(255, 203, 5, 0.3);
            border-radius: 15px;
            padding: 20px;
            margin-bottom: 25px;
        }

        .progress-bar-container {
            width: 100%;
            height: 8px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            overflow: hidden;
            margin-bottom: 10px;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #FFCB05 0%, #FFD740 100%);
            border-radius: 10px;
            transition: width 0.5s ease;
            width: 0%;
        }

        .progress-text {
            text-align: center;
            font-size: 0.9em;
            color: #B0B8C0;
        }

        .module-list {
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .module-card {
            background: rgba(20, 21, 23, 0.8);
            border: 2px solid rgba(255, 203, 5, 0.2);
            border-radius: 15px;
            padding: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
        }

        .module-card.active {
            border-color: #FFCB05;
            background: rgba(255, 203, 5, 0.1);
        }

        .module-card.completed {
            border-color: rgba(76, 175, 80, 0.5);
            opacity: 0.7;
        }

        .module-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }

        .module-number {
            font-size: 0.8em;
            color: #FFCB05;
            font-weight: 600;
        }

        .module-status {
            font-size: 1.2em;
        }

        .module-title {
            font-size: 1.2em;
            font-weight: 600;
            margin-bottom: 8px;
            color: #FFFFFF;
        }

        .module-meta {
            display: flex;
            gap: 15px;
            font-size: 0.85em;
            color: #B0B8C0;
            margin-bottom: 10px;
        }

        .module-meta span {
            display: flex;
            align-items: center;
            gap: 5px;
        }

        .module-description {
            font-size: 0.9em;
            color: #B0B8C0;
            line-height: 1.5;
            margin-bottom: 15px;
        }

        .audio-player {
            background: rgba(0, 39, 76, 0.5);
            border-radius: 12px;
            padding: 15px;
            display: none;
        }

        .module-card.active .audio-player {
            display: block;
        }

        .audio-controls {
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .play-button {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background: linear-gradient(135deg, #FFCB05 0%, #FFD740 100%);
            border: none;
            color: #00274C;
            font-size: 1.5em;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.2s ease;
            flex-shrink: 0;
        }

        .play-button:active {
            transform: scale(0.95);
        }

        .audio-info {
            flex: 1;
        }

        .audio-timeline {
            width: 100%;
            height: 4px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 2px;
            margin-bottom: 5px;
            position: relative;
            cursor: pointer;
        }

        .audio-progress {
            height: 100%;
            background: #FFCB05;
            border-radius: 2px;
            width: 0%;
            transition: width 0.1s linear;
        }

        .audio-time {
            display: flex;
            justify-content: space-between;
            font-size: 0.75em;
            color: #B0B8C0;
        }

        .playback-controls {
            display: flex;
            gap: 10px;
            margin-top: 10px;
        }

        .control-button {
            flex: 1;
            padding: 10px;
            background: rgba(255, 203, 5, 0.1);
            border: 1px solid rgba(255, 203, 5, 0.3);
            border-radius: 8px;
            color: #FFCB05;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .control-button:active {
            background: rgba(255, 203, 5, 0.2);
        }

        .audio-speed {
            padding: 5px 10px;
            background: rgba(255, 203, 5, 0.1);
            border: 1px solid rgba(255, 203, 5, 0.3);
            border-radius: 8px;
            color: #FFCB05;
            font-size: 0.85em;
            text-align: center;
        }

        .fixed-nav {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(0, 39, 76, 0.95);
            backdrop-filter: blur(10px);
            border-top: 2px solid rgba(255, 203, 5, 0.3);
            padding: 15px;
            display: flex;
            gap: 10px;
            z-index: 1000;
        }

        .nav-button {
            flex: 1;
            padding: 15px;
            background: linear-gradient(135deg, #FFCB05 0%, #FFD740 100%);
            border: none;
            border-radius: 12px;
            color: #00274C;
            font-weight: 600;
            font-size: 1em;
            cursor: pointer;
            transition: transform 0.2s ease;
        }

        .nav-button:active {
            transform: scale(0.98);
        }

        .nav-button:disabled {
            opacity: 0.3;
            cursor: not-allowed;
        }

        .toast {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.9);
            color: white;
            padding: 15px 25px;
            border-radius: 12px;
            font-size: 0.9em;
            z-index: 2000;
            display: none;
            border: 1px solid rgba(255, 203, 5, 0.3);
        }

        .toast.show {
            display: block;
            animation: slideDown 0.3s ease;
        }

        @keyframes slideDown {
            from {
                transform: translateX(-50%) translateY(-20px);
                opacity: 0;
            }
            to {
                transform: translateX(-50%) translateY(0);
                opacity: 1;
            }
        }

        .pillar-header {
            background: rgba(255, 203, 5, 0.1);
            border: 2px solid rgba(255, 203, 5, 0.3);
            border-radius: 12px;
            padding: 15px;
            margin: 25px 0 15px;
        }

        .pillar-title {
            font-size: 1.3em;
            font-weight: 600;
            color: #FFCB05;
            margin-bottom: 5px;
        }

        .pillar-subtitle {
            font-size: 0.9em;
            color: #B0B8C0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="logo">M</div>
            <h1>Chief Data & AI Officer</h1>
            <div class="subtitle">University of Michigan Professional Certificate</div>
            <div class="stats">
                <div class="stat">
                    <div class="stat-number">12</div>
                    <div class="stat-label">Sessions</div>
                </div>
                <div class="stat">
                    <div class="stat-number">58</div>
                    <div class="stat-label">Hours</div>
                </div>
                <div class="stat">
                    <div class="stat-number">5</div>
                    <div class="stat-label">Months</div>
                </div>
            </div>
        </header>

        <div class="progress-overview">
            <div class="progress-bar-container">
                <div class="progress-bar" id="overallProgress"></div>
            </div>
            <div class="progress-text" id="progressText">0 of 12 sessions completed (0%)</div>
        </div>

        <!-- Pillar 1: Foundations of Value Creation -->
        <div class="pillar-header">
            <div class="pillar-title">Pillar 1: Foundations of Value Creation</div>
            <div class="pillar-subtitle">Sessions 1-4 • Weeks 1-8</div>
        </div>

        <div class="module-list">
            <!-- Session 1 -->
            <div class="module-card" data-module="1">
                <div class="module-header">
                    <div class="module-number">SESSION 1</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">AI Past, Present & Future I</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 1</span>
                    <span>🎯 Foundations</span>
                </div>
                <div class="module-description">
                    Learn the evolution of AI from symbolic systems to foundation models. Understand machine learning fundamentals and current AI capabilities.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 2 -->
            <div class="module-card" data-module="2">
                <div class="module-header">
                    <div class="module-number">SESSION 2</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">AI Past, Present & Future II</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 2</span>
                    <span>🎯 Foundations</span>
                </div>
                <div class="module-description">
                    Deep dive into transfer learning and foundation model selection. Explore GPT-4, Claude, and open-source alternatives for enterprise use.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 3 -->
            <div class="module-card" data-module="3">
                <div class="module-header">
                    <div class="module-number">SESSION 3</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Data Strategy for Success I</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 4</span>
                    <span>🎯 Strategy</span>
                </div>
                <div class="module-description">
                    Master enterprise data strategy frameworks. Learn to align data initiatives with business objectives and build data-driven culture.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 4 -->
            <div class="module-card" data-module="4">
                <div class="module-header">
                    <div class="module-number">SESSION 4</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Data Strategy for Success II</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 6</span>
                    <span>🎯 Architecture</span>
                </div>
                <div class="module-description">
                    Explore data architectures: data mesh, data fabric, and cloud platforms. Design scalable data pipelines and modern data stacks.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>
        </div>

        <!-- Pillar 2: Business Strategy -->
        <div class="pillar-header">
            <div class="pillar-title">Pillar 2: Business Strategy & Value</div>
            <div class="pillar-subtitle">Sessions 5-8 • Weeks 9-12</div>
        </div>

        <div class="module-list">
            <!-- Session 5 -->
            <div class="module-card" data-module="5">
                <div class="module-header">
                    <div class="module-number">SESSION 5</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">AI Strategy & Digital Transformation</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 9</span>
                    <span>🎯 Strategy</span>
                </div>
                <div class="module-description">
                    Develop AI strategy aligned with digital transformation goals. Learn ROI frameworks and build/buy/partner decision models.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 6 -->
            <div class="module-card" data-module="6">
                <div class="module-header">
                    <div class="module-number">SESSION 6</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">AI Business Applications & Use Cases</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 10</span>
                    <span>🎯 Applications</span>
                </div>
                <div class="module-description">
                    Discover AI applications across industries: predictive analytics, recommendation systems, NLP, computer vision, and autonomous systems.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 7 -->
            <div class="module-card" data-module="7">
                <div class="module-header">
                    <div class="module-number">SESSION 7</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Leading Data & Analytics Teams</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 11</span>
                    <span>🎯 Leadership</span>
                </div>
                <div class="module-description">
                    Build and lead high-performing data teams. Learn org structures, talent acquisition, skills development, and performance management.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 8 -->
            <div class="module-card" data-module="8">
                <div class="module-header">
                    <div class="module-number">SESSION 8</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">AI Governance, Ethics & Trust</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 12</span>
                    <span>🎯 Governance</span>
                </div>
                <div class="module-description">
                    Master AI governance frameworks. Address bias, fairness, transparency, privacy, and regulatory compliance (GDPR, AI Act).
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>
        </div>

        <!-- Pillar 3: Leadership & Culture -->
        <div class="pillar-header">
            <div class="pillar-title">Pillar 3: Leadership & Change Management</div>
            <div class="pillar-subtitle">Sessions 9-10 • Weeks 13-16</div>
        </div>

        <div class="module-list">
            <!-- Session 9 -->
            <div class="module-card" data-module="9">
                <div class="module-header">
                    <div class="module-number">SESSION 9</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Building AI-Ready Organizations</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 13</span>
                    <span>🎯 Culture</span>
                </div>
                <div class="module-description">
                    Transform organizational culture for AI adoption. Drive change management, overcome resistance, and build data-driven mindsets.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 10 -->
            <div class="module-card" data-module="10">
                <div class="module-header">
                    <div class="module-number">SESSION 10</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Communicating AI to Stakeholders</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 15</span>
                    <span>🎯 Communication</span>
                </div>
                <div class="module-description">
                    Learn to communicate technical concepts to executives, boards, and non-technical stakeholders. Build business cases for AI investments.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>
        </div>

        <!-- Pillar 4: Future of AI -->
        <div class="pillar-header">
            <div class="pillar-title">Pillar 4: Future & Innovation</div>
            <div class="pillar-subtitle">Sessions 11-12 • Weeks 17-18</div>
        </div>

        <div class="module-list">
            <!-- Session 11 -->
            <div class="module-card" data-module="11">
                <div class="module-header">
                    <div class="module-number">SESSION 11</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">Future of AI & Emerging Technologies</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 17</span>
                    <span>🎯 Innovation</span>
                </div>
                <div class="module-description">
                    Explore cutting-edge AI: multimodal models, AI agents, quantum computing, AGI, and emerging technology convergence.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>

            <!-- Session 12 -->
            <div class="module-card" data-module="12">
                <div class="module-header">
                    <div class="module-number">SESSION 12</div>
                    <div class="module-status">⏸️</div>
                </div>
                <div class="module-title">CDAIO Role & Career Development</div>
                <div class="module-meta">
                    <span>⏱️ 30 min</span>
                    <span>📍 Week 18</span>
                    <span>🎯 Career</span>
                </div>
                <div class="module-description">
                    Define the modern CDAIO role. Build your career roadmap, personal brand, and leadership presence in the AI executive space.
                </div>
                <div class="audio-player">
                    <div class="audio-controls">
                        <button class="play-button" data-action="play">▶️</button>
                        <div class="audio-info">
                            <div class="audio-timeline" data-action="seek">
                                <div class="audio-progress"></div>
                            </div>
                            <div class="audio-time">
                                <span class="current-time">0:00</span>
                                <span class="total-time">30:00</span>
                            </div>
                        </div>
                    </div>
                    <div class="playback-controls">
                        <button class="control-button" data-action="rewind">⏪ -15s</button>
                        <div class="audio-speed">1.0x</div>
                        <button class="control-button" data-action="forward">+15s ⏩</button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Fixed Navigation -->
    <div class="fixed-nav">
        <button class="nav-button" id="prevBtn" disabled>⬅️ Previous</button>
        <button class="nav-button" id="nextBtn">Next ➡️</button>
    </div>

    <!-- Toast Notifications -->
    <div class="toast" id="toast"></div>

    <script>
        // Module content - actual curriculum from UMich CDAIO Implementation Blueprint
        const moduleContent = {
            1: {
                title: "AI Past, Present & Future I",
                script: "Welcome to Session 1: AI Past, Present and Future. In this session, we'll explore the evolution of artificial intelligence from symbolic AI systems to machine learning, deep learning, and foundation models. We'll cover machine learning fundamentals including supervised learning, unsupervised learning, and reinforcement learning. You'll understand current AI capabilities and limitations, and get an overview of foundation models like GPT, BERT, and Stable Diffusion. The session includes a 60-minute lecture, 20-minute Q&A, and 10-minute breakout to discuss AI challenges your organization is facing. After this session, you'll complete a quiz on AI fundamentals and have the option to train a simple machine learning model in Colab."
            },
            2: {
                title: "AI Past, Present & Future II",
                script: "Welcome to Session 2: AI Past, Present and Future Part 2. Today we dive deep into transfer learning and pre-trained models. We'll explore fine-tuning strategies and compare foundation models including GPT-4, Claude, and open-source alternatives. We'll discuss future AI trends including AGI, autonomous agents, and multimodal AI. You'll analyze a case study on how Netflix uses transfer learning for recommendations. In the workshop portion, you'll compare three foundation models for your specific use case. Your assignment is to identify three foundation model use cases for your organization."
            },
            3: {
                title: "Data Strategy for Success I",
                script: "Welcome to Session 3: Data Strategy for Success. This session focuses on enterprise data strategy frameworks and how to align data strategy to business objectives. We'll cover data quality, data governance, and building a data-driven culture. You'll learn from the McKinsey Data-Driven Enterprise report and complete a data maturity self-assessment. The workshop will guide you through drafting a data strategy outline using our provided template. Your graded deliverable is a Data Strategy 1-Pager for your organization, worth 10 points."
            },
            4: {
                title: "Data Strategy for Success II",
                script: "Welcome to Session 4: Data Strategy for Success Continued. We'll explore different data architectures including centralized, federated, and data mesh approaches. You'll learn about cloud data platforms on AWS, Azure, and GCP. We'll cover data pipelines, ETL processes, and introduce DataOps and MLOps fundamentals. In the architecture workshop, you'll sketch your current versus future state data architecture. This session includes an optional lab where you can build a simple data pipeline in Colab."
            },
            5: {
                title: "AI Strategy & Digital Transformation",
                script: "Welcome to Session 5: AI and Business Strategy. This session covers AI strategy frameworks and digital transformation roadmaps. We'll explore business model innovation with AI and change management for AI adoption. You'll analyze Amazon's AI strategy as a case study and work with our AI Transformation Roadmap Canvas. In the workshop, you'll map AI opportunities to your business model. Your graded deliverable is an AI Transformation 1-Pager worth 25 points."
            },
            6: {
                title: "AI Business Applications & Use Cases",
                script: "Welcome to Session 6: Leverage AI in Operations. This session focuses on operational efficiency with AI, process automation, quality improvement through machine learning, and achieving speed and cost reduction. You'll work with our process automation ROI calculator during the workshop. We'll examine real-world examples across industries including predictive analytics, recommendation systems, natural language processing, computer vision, and autonomous systems."
            },
            7: {
                title: "Leading Data & Analytics Teams",
                script: "Welcome to Session 7: Leverage AI for Customers. Today we explore customer engagement AI, personalization engines, customer service automation, and predictive analytics for behavior. The workshop focuses on customer journey mapping with AI touchpoints. Your graded deliverable is a Customer AI Strategy Proposal worth 15 points. We'll examine how leading companies use AI to transform customer experiences."
            },
            8: {
                title: "AI Governance, Ethics & Trust",
                script: "Welcome to Session 8: Leverage AI for Decision Making. This session covers AI-augmented decision making, organizational design for AI, and AI Center of Excellence structures. We'll discuss building an AI-ready culture in your organization. The workshop guides you through designing an AI CoE for your specific organization. Your deliverable is an AI CoE Proposal worth 15 points."
            },
            9: {
                title: "Building AI-Ready Organizations",
                script: "Welcome to Session 9: System Integrity and Trust. This critical session covers data governance frameworks, GDPR, CCPA, and HIPAA compliance. We'll explore AI trustworthiness, transparency, and explainability. The workshop includes a GDPR compliance audit for AI systems. Your graded deliverable is a Data Governance Policy Draft worth 15 points. This is essential knowledge for any Chief Data and AI Officer."
            },
            10: {
                title: "Communicating AI to Stakeholders",
                script: "Welcome to Session 10: Securing Your System. Today we cover data security fundamentals, encryption strategies, anonymization techniques, and secure machine learning pipelines. We'll practice threat modeling with a guest speaker who is a CISO from a Fortune 500 company. The workshop focuses on threat modeling for your AI system. Your Security Architecture Proposal deliverable is worth 15 points."
            },
            11: {
                title: "Future of AI & Emerging Technologies",
                script: "Welcome to Session 11: Risk Management. This session introduces AI risk frameworks, model risk management, business continuity for AI, and incident response. We'll conduct an AI incident response tabletop exercise simulation. Your Risk Assessment deliverable is worth 15 points. This session prepares you to identify and mitigate AI risks in your organization."
            },
            12: {
                title: "CDAIO Role & Career Development",
                script: "Welcome to Session 12: Ethical AI. Our final session covers responsible AI principles, bias detection and mitigation, fairness in machine learning, and privacy-preserving AI. We'll examine ethical frameworks with a bioethics expert and engage in a debate: Should we deploy this AI system? Your Responsible AI Framework deliverable is worth 15 points. This caps off your journey through the four pillars of the Chief Data and AI Officer program. You're now ready for the 5-day immersion week where you'll develop your capstone project."
            }
        };

        // State management
        let currentModule = null;
        let currentAudio = null;
        let speechSynthesis = window.speechSynthesis;
        let currentUtterance = null;
        let playbackSpeed = 1.0;
        let moduleProgress = {};
        let audioStartTime = 0;
        let audioPauseTime = 0;
        let audioInterval = null;

        // Initialize from localStorage
        function initializeProgress() {
            const saved = localStorage.getItem('umich_cdaio_progress');
            if (saved) {
                moduleProgress = JSON.parse(saved);
                updateUI();
            }
        }

        // Save progress to localStorage
        function saveProgress() {
            localStorage.setItem('umich_cdaio_progress', JSON.stringify(moduleProgress));
            updateOverallProgress();
        }

        // Update overall progress
        function updateOverallProgress() {
            const completed = Object.values(moduleProgress).filter(p => p.completed).length;
            const total = 12;
            const percentage = Math.round((completed / total) * 100);

            document.getElementById('overallProgress').style.width = percentage + '%';
            document.getElementById('progressText').textContent =
                `${completed} of ${total} sessions completed (${percentage}%)`;
        }

        // Show toast notification
        function showToast(message) {
            const toast = document.getElementById('toast');
            toast.textContent = message;
            toast.classList.add('show');
            setTimeout(() => toast.classList.remove('show'), 2000);
        }

        // Module card click handler
        document.querySelectorAll('.module-card').forEach(card => {
            card.addEventListener('click', function(e) {
                // Don't toggle if clicking on player controls
                if (e.target.closest('.audio-player')) return;

                const moduleId = this.dataset.module;

                // Close current module if different
                if (currentModule && currentModule !== moduleId) {
                    document.querySelector(`[data-module="${currentModule}"]`).classList.remove('active');
                    stopAudio();
                }

                // Toggle current module
                this.classList.toggle('active');
                currentModule = this.classList.contains('active') ? moduleId : null;

                updateNavButtons();
            });
        });

        // Play/Pause button
        document.querySelectorAll('[data-action="play"]').forEach(btn => {
            btn.addEventListener('click', function(e) {
                e.stopPropagation();
                const card = this.closest('.module-card');
                const moduleId = card.dataset.module;

                if (this.textContent === '▶️') {
                    playAudio(moduleId, this);
                } else {
                    pauseAudio(this);
                }
            });
        });

        // Rewind button - Note: Web Speech API doesn't support seeking
        document.querySelectorAll('[data-action="rewind"]').forEach(btn => {
            btn.addEventListener('click', function(e) {
                e.stopPropagation();
                showToast('⏪ Rewind not supported in speech mode');
            });
        });

        // Forward button - Note: Web Speech API doesn't support seeking
        document.querySelectorAll('[data-action="forward"]').forEach(btn => {
            btn.addEventListener('click', function(e) {
                e.stopPropagation();
                showToast('⏩ Forward not supported in speech mode');
            });
        });

        // Speed button
        document.querySelectorAll('.audio-speed').forEach(btn => {
            btn.addEventListener('click', function(e) {
                e.stopPropagation();
                const speeds = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0];
                const currentIndex = speeds.indexOf(playbackSpeed);
                playbackSpeed = speeds[(currentIndex + 1) % speeds.length];
                this.textContent = playbackSpeed + 'x';
                if (currentUtterance && speechSynthesis.speaking) {
                    // Update current utterance rate
                    currentUtterance.rate = playbackSpeed;
                }
                showToast(`Playback speed: ${playbackSpeed}x`);
            });
        });

        // Seek functionality
        document.querySelectorAll('[data-action="seek"]').forEach(timeline => {
            timeline.addEventListener('click', function(e) {
                e.stopPropagation();
                if (!currentAudio) return;

                const rect = this.getBoundingClientRect();
                const percent = (e.clientX - rect.left) / rect.width;
                currentAudio.currentTime = percent * currentAudio.duration;
            });
        });

        // Play audio using Web Speech API
        function playAudio(moduleId, button) {
            // Stop any existing speech
            speechSynthesis.cancel();

            // Get module content
            const content = moduleContent[moduleId];
            if (!content) {
                showToast('Module content not available');
                return;
            }

            // Create speech utterance
            currentUtterance = new SpeechSynthesisUtterance(content.script);
            currentUtterance.rate = playbackSpeed;
            currentUtterance.pitch = 1.0;
            currentUtterance.volume = 1.0;

            // Select best voice (prefer English voices)
            const voices = speechSynthesis.getVoices();
            const preferredVoice = voices.find(v => v.lang.startsWith('en') && v.name.includes('Samantha')) ||
                                  voices.find(v => v.lang.startsWith('en-US')) ||
                                  voices[0];
            if (preferredVoice) {
                currentUtterance.voice = preferredVoice;
            }

            // Set up event handlers
            currentUtterance.onend = () => {
                completeModule(moduleId);
                stopAudio();
                button.textContent = '▶️';
                const card = button.closest('.module-card');
                card.querySelector('.module-status').textContent = '✅';
            };

            currentUtterance.onerror = (event) => {
                console.error('Speech synthesis error:', event);
                showToast('Audio error: ' + event.error);
                stopAudio();
            };

            // Start speaking
            speechSynthesis.speak(currentUtterance);

            // Update UI
            button.textContent = '⏸️';
            const card = button.closest('.module-card');
            card.querySelector('.module-status').textContent = '▶️';

            // Start progress tracking
            audioStartTime = Date.now();
            const scriptLength = content.script.length;
            const estimatedDuration = (scriptLength / 15) * (1 / playbackSpeed); // ~15 chars per second

            audioInterval = setInterval(() => {
                if (speechSynthesis.speaking) {
                    const elapsed = (Date.now() - audioStartTime) / 1000;
                    const progress = Math.min((elapsed / estimatedDuration) * 100, 99);

                    const progressBar = card.querySelector('.audio-progress');
                    const currentTime = card.querySelector('.current-time');

                    progressBar.style.width = progress + '%';
                    currentTime.textContent = formatTime(elapsed);
                }
            }, 100);

            showToast('🎧 Playing audio narration');
        }

        // Pause audio
        function pauseAudio(button) {
            if (speechSynthesis.speaking) {
                speechSynthesis.pause();
                button.textContent = '▶️';
                const card = button.closest('.module-card');
                card.querySelector('.module-status').textContent = '⏸️';

                if (audioInterval) {
                    clearInterval(audioInterval);
                    audioInterval = null;
                }

                // Save progress
                const moduleId = card.dataset.module;
                if (!moduleProgress[moduleId]) {
                    moduleProgress[moduleId] = {};
                }
                moduleProgress[moduleId].position = (Date.now() - audioStartTime) / 1000;
                saveProgress();

                showToast('Audio paused');
            } else if (speechSynthesis.paused) {
                // Resume
                speechSynthesis.resume();
                button.textContent = '⏸️';
                const card = button.closest('.module-card');
                card.querySelector('.module-status').textContent = '▶️';

                // Restart progress tracking
                const moduleId = card.dataset.module;
                const content = moduleContent[moduleId];
                const scriptLength = content.script.length;
                const estimatedDuration = (scriptLength / 15) * (1 / playbackSpeed);

                audioInterval = setInterval(() => {
                    if (speechSynthesis.speaking) {
                        const elapsed = (Date.now() - audioStartTime) / 1000;
                        const progress = Math.min((elapsed / estimatedDuration) * 100, 99);

                        const progressBar = card.querySelector('.audio-progress');
                        const currentTime = card.querySelector('.current-time');

                        progressBar.style.width = progress + '%';
                        currentTime.textContent = formatTime(elapsed);
                    }
                }, 100);

                showToast('Audio resumed');
            }
        }

        // Stop audio
        function stopAudio() {
            speechSynthesis.cancel();

            if (audioInterval) {
                clearInterval(audioInterval);
                audioInterval = null;
            }

            currentUtterance = null;

            document.querySelectorAll('[data-action="play"]').forEach(btn => {
                btn.textContent = '▶️';
            });
        }

        // Format time
        function formatTime(seconds) {
            const mins = Math.floor(seconds / 60);
            const secs = Math.floor(seconds % 60);
            return `${mins}:${secs.toString().padStart(2, '0')}`;
        }

        // Complete module
        function completeModule(moduleId) {
            moduleProgress[moduleId] = {
                completed: true,
                position: 0
            };
            saveProgress();

            const card = document.querySelector(`[data-module="${moduleId}"]`);
            card.classList.add('completed');
            card.querySelector('.module-status').textContent = '✅';

            showToast('Session completed! 🎉');
        }

        // Navigation buttons
        document.getElementById('prevBtn').addEventListener('click', () => {
            if (!currentModule) return;
            const prev = parseInt(currentModule) - 1;
            if (prev >= 1) {
                openModule(prev);
            }
        });

        document.getElementById('nextBtn').addEventListener('click', () => {
            if (!currentModule) {
                openModule(1);
            } else {
                const next = parseInt(currentModule) + 1;
                if (next <= 12) {
                    openModule(next);
                }
            }
        });

        // Open specific module
        function openModule(moduleId) {
            // Close current
            if (currentModule) {
                document.querySelector(`[data-module="${currentModule}"]`).classList.remove('active');
                stopAudio();
            }

            // Open new
            const card = document.querySelector(`[data-module="${moduleId}"]`);
            card.classList.add('active');
            card.scrollIntoView({ behavior: 'smooth', block: 'center' });
            currentModule = moduleId.toString();

            updateNavButtons();
        }

        // Update navigation buttons
        function updateNavButtons() {
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');

            if (!currentModule) {
                prevBtn.disabled = true;
                nextBtn.disabled = false;
            } else {
                const num = parseInt(currentModule);
                prevBtn.disabled = num === 1;
                nextBtn.disabled = num === 12;
            }
        }

        // Update UI based on saved progress
        function updateUI() {
            Object.entries(moduleProgress).forEach(([moduleId, progress]) => {
                const card = document.querySelector(`[data-module="${moduleId}"]`);
                if (progress.completed) {
                    card.classList.add('completed');
                    card.querySelector('.module-status').textContent = '✅';
                }
            });
        }

        // Initialize
        initializeProgress();
        updateOverallProgress();
        updateNavButtons();

        // Load voices (required for some browsers/iOS)
        let voicesLoaded = false;
        function loadVoices() {
            const voices = speechSynthesis.getVoices();
            if (voices.length > 0 && !voicesLoaded) {
                voicesLoaded = true;
                console.log('Voices loaded:', voices.length);
            }
        }

        // iOS requires voices to be loaded
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = loadVoices;
        }
        loadVoices();

        // Add iOS audio support
        document.addEventListener('DOMContentLoaded', () => {
            // Enable audio on iOS
            if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
                showToast('🎓 Welcome to UMich CDAIO! Tap any session to begin');
                // Preload voices for iOS
                loadVoices();
            } else {
                showToast('🎓 Welcome! Click any session to start audio training');
            }
        });
    </script>
</body>
</html>
// Extended 30-minute narration scripts for UMich CDAIO sessions
// Each script is approximately 4,500 words for 30 minutes of audio

const extendedModuleContent = {
    1: {
        title: "AI Past, Present & Future I",
        duration: 30,
        script: `Welcome to Session 1 of the Chief Data and AI Officer program: AI Past, Present, and Future, Part 1. I'm thrilled to guide you through this comprehensive exploration of artificial intelligence, from its inception to the cutting-edge systems transforming our world today.

Let's begin our journey in 1956 at Dartmouth College in New Hampshire. That summer, John McCarthy, Marvin Minsky, Claude Shannon, and Nathan Rochester organized a workshop that would birth an entirely new field. They proposed the hypothesis that every aspect of learning or any other feature of intelligence can, in principle, be so precisely described that a machine can be made to simulate it. This audacious claim marked the beginning of AI as an academic discipline.

The pioneers of AI were optimistic, perhaps overly so. In 1958, Herbert Simon predicted that within ten years, a computer would be the world's chess champion. In 1965, Simon said machines would be capable of doing any work a human can do within twenty years. These predictions, though premature, reflected genuine excitement about the possibilities.

The early years focused on symbolic AI, sometimes called GOFAI, Good Old Fashioned AI. The fundamental idea was elegant: if we can represent knowledge as symbols and manipulate those symbols according to logical rules, we can create intelligent machines. Programs like the Logic Theorist proved mathematical theorems. ELIZA, created by Joseph Weizenbaum in 1966, simulated a Rogerian psychotherapist through pattern matching and substitution, fooling some users into believing they were conversing with a human.

Expert systems emerged as the first commercially successful AI application. MYCIN, developed at Stanford in the 1970s, diagnosed bacterial infections and recommended antibiotics, often performing as well as human experts. DENDRAL identified molecular structures from mass spectrometry data. These systems encoded domain expertise as if-then rules: if the patient has a fever and an elevated white blood cell count, then consider bacterial infection.

But symbolic AI had fundamental limitations. These systems were brittle, failing catastrophically when encountering situations outside their programmed rules. They couldn't handle uncertainty or ambiguous data. Building them required painstaking knowledge engineering, interviewing experts and manually encoding their knowledge. Scaling was nearly impossible. And they couldn't learn from experience.

By the mid-1970s, these limitations became undeniable. The British government's Lighthill Report of 1973 was devastating, concluding that AI had failed to achieve its grandiose objectives. Funding dried up. Researchers moved to other fields. This period, from the mid-1970s to early 1980s, became known as the first AI winter.

The 1980s brought renewed hope with the commercial success of expert systems. Companies like Digital Equipment Corporation deployed XCON, which configured computer systems, saving millions of dollars annually. Japanese companies launched the Fifth Generation Computer Project, aiming to build intelligent computers. American corporations invested billions in AI, creating entire departments dedicated to expert systems.

But by the late 1980s, disillusionment returned. Expert systems proved expensive to develop and maintain. They couldn't adapt to changing conditions. When the underlying domain knowledge shifted, the entire system needed rebuilding. Desktop computers became powerful enough to perform many tasks previously requiring specialized AI hardware, making those investments obsolete. The second AI winter arrived, lasting through the early 1990s.

The renaissance began quietly with a paradigm shift: instead of programming intelligence, what if machines could learn from data? This wasn't entirely new; Arthur Samuel had created a checkers-playing program in 1959 that improved through self-play. But by the 1990s, increases in computing power and data availability made machine learning practical.

Let me explain the three fundamental paradigms of machine learning. First, supervised learning. Imagine you're teaching a child to recognize different types of fruit. You show them an apple and say, "This is an apple." You show them an orange and say, "This is an orange." After enough examples, the child can identify fruit they've never seen before. Supervised learning works the same way.

You provide a training dataset consisting of input-output pairs. For image classification, inputs are images and outputs are labels like "cat," "dog," or "bird." For spam detection, inputs are emails and outputs are "spam" or "not spam." The algorithm learns a function mapping inputs to outputs by finding patterns in the training data.

Different algorithms approach this differently. Decision trees split data based on feature values, creating a tree-like model of decisions. If you're predicting whether someone will default on a loan, you might split first on income, then on credit score, then on employment history. Random forests improve on this by training many decision trees on random subsets of data and averaging their predictions, reducing overfitting.

Support vector machines find the optimal hyperplane that maximizes the margin between different classes. Imagine points in space representing different data, with some points colored red and others colored blue. An SVM finds the boundary that separates red from blue with the maximum cushion on either side.

Neural networks, inspired by biological brains, consist of layers of interconnected nodes. Each connection has a weight that determines how much influence one node has on another. During training, these weights adjust to minimize the difference between predictions and actual outcomes. We'll discuss neural networks in much more detail shortly.

The second paradigm is unsupervised learning. Here, you give the algorithm unlabeled data and ask it to discover hidden structure. It's like giving a child a box of mixed toys and asking them to organize it however makes sense. They might group toy cars together, dolls together, building blocks together, discovering natural categories without being told what they are.

Clustering algorithms like K-means partition data into groups. In customer segmentation, you might discover that your customers naturally fall into categories: price-sensitive buyers, premium seekers, convenience-focused shoppers. The algorithm finds these groups automatically based on purchasing behavior, demographics, and other features.

Dimensionality reduction techniques like principal component analysis identify the most important features in high-dimensional data. If you're analyzing houses with hundreds of features, PCA might reveal that most variation comes from just a few factors like location, size, and age. This makes data easier to visualize and models faster to train.

Anomaly detection identifies unusual patterns that don't conform to expected behavior. In fraud detection, most transactions follow predictable patterns. Anomalous transactions, those that deviate significantly, warrant investigation. In network security, anomaly detection identifies potential intrusions by spotting unusual traffic patterns.

The third paradigm is reinforcement learning. This is how we teach agents to make sequences of decisions to achieve goals. Think about training a dog. You want to teach them to sit on command. You give the command, and when they sit, you provide a treat. If they don't sit, no treat. Over time, the dog learns that sitting when commanded leads to rewards.

Reinforcement learning formalizes this as an agent interacting with an environment. At each time step, the agent observes the environment's state, chooses an action, and receives a reward. The goal is to learn a policy, a strategy for selecting actions that maximizes cumulative reward over time.

This framework is remarkably general. In game playing, states are board positions, actions are legal moves, and rewards are winning or losing. In robotics, states are sensor readings, actions are motor commands, and rewards are task completion. In recommendation systems, states are user histories, actions are items to recommend, and rewards are user engagement.

Q-learning, one of the fundamental reinforcement learning algorithms, learns the quality of actions in each state. The Q-value represents expected future reward for taking a specific action in a specific state. The agent explores by trying different actions, observes the rewards, and updates Q-values accordingly. Over time, it learns which actions are best in which situations.

Now we arrive at deep learning, the breakthrough that revolutionized AI. Deep learning uses neural networks with many layers, allowing them to learn hierarchical representations automatically. But neural networks aren't new; they've been around since the 1950s. What changed?

Three factors converged in the 2010s. First, we gained access to massive datasets. ImageNet contains over 14 million labeled images. The internet provided billions of documents for language models. This abundance of data gave neural networks enough examples to learn effectively.

Second, computing power increased dramatically. Graphics processing units, originally designed for rendering video game graphics, turned out to be perfect for the parallel computations in neural networks. Training models that once took months now took days or hours.

Third, algorithmic innovations solved longstanding problems. The vanishing gradient problem, where gradients became too small for learning in deep networks, was mitigated by ReLU activation functions and batch normalization. Dropout regularization prevented overfitting. Better optimization algorithms like Adam improved training stability.

The breakthrough moment came in 2012 at the ImageNet Large Scale Visual Recognition Challenge. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton entered AlexNet, a deep convolutional neural network. It achieved a top-5 error rate of 15.3%, compared to 26.2% for the second-place entry. This margin was unprecedented and shocking.

Convolutional neural networks are specifically designed for image analysis. Unlike fully connected networks where every neuron connects to every neuron in the previous layer, convolutional networks use local connections and weight sharing. A convolutional filter slides across the image, detecting features like edges, textures, or patterns. Early layers detect simple features like horizontal and vertical edges. Middle layers combine these into more complex patterns like corners or curves. Deep layers recognize high-level features like eyes, wheels, or fur. Finally, fully connected layers at the end make the final classification.

This hierarchical feature learning happens automatically through training. You don't manually engineer features; the network learns them from data. This was revolutionary. In traditional computer vision, experts spent years developing feature extractors like SIFT or HOG. Deep learning made this obsolete.

The success in computer vision spread rapidly. In speech recognition, deep learning models surpassed traditional approaches. In natural language processing, recurrent neural networks and later transformers achieved state-of-the-art results. In game playing, DeepMind's AlphaGo used deep reinforcement learning to defeat the world champion in Go, a game long considered beyond computer capabilities.

This brings us to foundation models, the latest paradigm shift. Foundation models are large neural networks trained on massive, diverse datasets. Unlike traditional models trained for specific tasks, foundation models learn general representations that transfer to many downstream tasks.

GPT, Generative Pre-trained Transformer, exemplifies this approach. GPT-3, released in 2020, was trained on hundreds of billions of words from books, websites, and other text sources. It has 175 billion parameters, making it one of the largest neural networks ever created. During pre-training, GPT learns to predict the next word in sequences, forcing it to develop deep understanding of language, facts about the world, and even reasoning capabilities.

What makes foundation models powerful is transfer learning. After pre-training on general data, you can fine-tune them on specific tasks with relatively little task-specific data. Want a customer service chatbot? Fine-tune GPT on customer service conversations. Want a legal document analyzer? Fine-tune on legal texts. This approach is far more efficient than training from scratch.

BERT, Bidirectional Encoder Representations from Transformers, took a different approach. While GPT reads text left to right, BERT reads bidirectionally, using context from both directions simultaneously. This bidirectional attention dramatically improved performance on understanding tasks like question answering and sentiment analysis.

For images, models like CLIP learn joint representations of images and text by training on millions of image-caption pairs. This allows zero-shot classification: you can describe a category in natural language, and CLIP can identify images of that category without any examples. Stable Diffusion and DALL-E generate images from text descriptions, learning the relationship between language and visual concepts.

Current AI capabilities are impressive. Computer vision systems diagnose diseases from medical images, sometimes matching or exceeding radiologist accuracy. They detect cancer, classify diabetic retinopathy, and identify fractures. Speech recognition systems transcribe conversations in real-time with over 95% accuracy. Language models answer questions, write code, and generate creative content.

AI is transforming scientific research. AlphaFold solved the protein folding problem, predicting 3D structures from amino acid sequences. This will accelerate drug discovery tremendously. AI discovers new materials, optimizes chemical reactions, analyzes astronomical data, and predicts weather patterns.

But AI has significant limitations we must understand. Current systems lack common sense reasoning. They don't understand causality as humans do. They're brittle, failing unpredictably on inputs slightly different from training data. A self-driving car that performs perfectly in sunny weather might fail in snow. An image classifier might be fooled by imperceptible perturbations.

AI systems struggle with explainability. Deep neural networks are black boxes; their decision-making process is opaque. When a loan application is denied by an AI system, can we explain why? When a medical diagnosis system recommends treatment, can doctors trust its reasoning?

They're data-hungry. Humans learn new concepts from a few examples; children learn what cats are after seeing a handful. AI systems typically need thousands or millions of training examples. Few-shot learning remains a major challenge.

They don't transfer knowledge flexibly across domains. A chess AI can't play checkers without retraining. A system that recognizes objects in photos struggles with drawings or paintings. Humans generalize much more effectively.

Energy efficiency is concerning. The human brain operates on about 20 watts, less than a light bulb, yet vastly outperforms AI systems requiring megawatts. Training large models like GPT-3 generates significant carbon emissions. As AI scales, sustainability becomes critical.

Looking forward, researchers are exploring exciting directions. Multimodal models integrate vision, language, audio, and other modalities more seamlessly, matching how humans perceive the world through multiple senses simultaneously. Self-supervised learning reduces dependence on labeled data by learning from the inherent structure of data itself. Neuro-symbolic AI combines neural networks with symbolic reasoning, aiming to capture both pattern recognition and logical reasoning. Neuromorphic computing mimics brain architecture more closely, promising dramatic efficiency gains.

As Chief Data and AI Officers, you must balance optimism with realism. AI offers tremendous opportunities for value creation, but it's not magic. Understanding its capabilities and limitations helps you identify high-value applications, set appropriate expectations with stakeholders, and deploy AI responsibly and effectively.

In our next session, we'll dive deeper into transfer learning, explore foundation model selection, and discuss future trends including artificial general intelligence and autonomous agents. For now, reflect on one AI challenge in your organization. Consider which machine learning paradigm might address it, what data you'd need, and what limitations you might encounter.

Thank you for your attention. Please complete the quiz on AI fundamentals to reinforce these concepts. If time allows, try the optional lab where you'll train a simple machine learning model in Google Colab. I look forward to seeing you in Session 2.`
    },
    2: {
        title: "AI Past, Present & Future II",
        duration: 30,
        script: `Welcome back to Session 2: AI Past, Present, and Future, Part 2. In our first session, we covered the evolution from symbolic AI through machine learning to deep learning and foundation models. Today, we'll dive deeper into transfer learning, compare foundation models, and explore future trends that will shape your work as Chief Data and AI Officers.

Let's start with transfer learning, one of the most important breakthroughs enabling practical AI deployment. The key insight is elegant: knowledge learned for one task can be reused for related tasks. This mirrors how humans learn. When you learn to ride a bicycle, that knowledge transfers to riding a motorcycle. When you learn French, it makes learning Spanish easier because they share common linguistic structures.

In traditional machine learning, every task required training from scratch with task-specific data. If you wanted an image classifier for medical images, you'd need tens of thousands of labeled medical images and weeks of training time. This approach worked but was expensive, slow, and required massive datasets.

Transfer learning changes this fundamentally. You start with a model pre-trained on a large, general dataset. For vision tasks, this might be ImageNet with millions of images across thousands of categories. For language tasks, it might be billions of words from books and websites. The pre-trained model has already learned useful general features. For images: edges, textures, shapes, object parts. For language: grammar, word relationships, factual knowledge, reasoning patterns.

Now, to adapt this pre-trained model to your specific task, you have several strategies. The simplest is feature extraction. You freeze the pre-trained model's weights and use it as a fixed feature extractor. Remove the final classification layer, add a new layer for your specific task, and train only this new layer on your task-specific data. This is fast and effective when your task is similar to the pre-training task and you have limited data.

Fine-tuning goes further. You initialize with pre-trained weights but continue training on your task-specific data, allowing the entire model or selected layers to adapt. Typically, you freeze early layers, which capture general features, and fine-tune later layers, which capture task-specific features. This balances retaining useful pre-trained knowledge with adapting to your specific domain.

Let me illustrate with a concrete example. Suppose you're building a skin cancer detection system. Training a deep neural network from scratch would require hundreds of thousands of labeled dermatology images and weeks of training on expensive GPUs. Instead, you take a model pre-trained on ImageNet. Even though ImageNet contains cats, dogs, and cars rather than skin lesions, the early layers have learned to detect edges, textures, and patterns that are universally useful for image analysis.

You replace the final classification layer with a new layer for your classes: benign, melanoma, basal cell carcinoma, squamous cell carcinoma. You fine-tune on a few thousand labeled dermatology images. Within hours, you have a model that performs remarkably well, often matching dermatologist-level accuracy. This is the power of transfer learning: standing on the shoulders of giants.

In natural language processing, BERT demonstrated transfer learning's potential. Pre-trained on massive text corpora through masked language modeling, where random words are hidden and the model learns to predict them, BERT develops deep linguistic understanding. Fine-tuning BERT on specific tasks like sentiment analysis, named entity recognition, or question answering achieves state-of-the-art results with relatively small task-specific datasets.

GPT takes a different approach called few-shot and zero-shot learning. Instead of fine-tuning, you provide task descriptions and a few examples directly in the prompt. For translation, you might write: "Translate English to French. English: Hello. French: Bonjour. English: Goodbye. French: Au revoir. English: Thank you. French:" GPT completes with "Merci." This in-context learning is remarkable because it requires no gradient updates or fine-tuning, just clever prompting.

Now let's compare foundation models, a critical skill for Chief Data and AI Officers. The landscape is rapidly evolving, but understanding the trade-offs helps you make informed decisions.

GPT-4, developed by OpenAI, is among the most capable language models available. It excels at complex reasoning, code generation, creative writing, and multi-turn conversation. It's multimodal, processing both text and images. However, it's proprietary, accessed only through API with usage costs. You have no control over the model architecture, training data, or fine-tuning. For sensitive applications requiring data privacy, sending data to external APIs may be unacceptable.

Claude, developed by Anthropic, emphasizes safety and reliability. It's designed to be helpful, harmless, and honest, with extensive alignment training to reduce harmful outputs. Claude excels at following instructions precisely and maintaining coherent long-form conversations. Like GPT-4, it's proprietary and API-based. The trade-off is similar: excellent performance but less control.

Open-source alternatives offer different trade-offs. LLaMA, developed by Meta, and Mistral, developed by Mistral AI, provide models you can download, host on your own infrastructure, and fine-tune freely. This gives complete control over data privacy, model behavior, and deployment. You can create custom versions tailored to your specific domain. The trade-offs are operational complexity, hosting costs, and sometimes lower performance than the largest proprietary models, though the gap is narrowing rapidly.

For vision tasks, the landscape differs. CLIP from OpenAI learns joint text-image representations, enabling zero-shot classification. You can classify images into categories the model has never seen by simply describing them in natural language. Stable Diffusion generates high-quality images from text prompts and is open-source, allowing you to run it locally and fine-tune it on your own images.

How do you choose? Consider several factors. First, task requirements. Does your task require cutting-edge performance, or is good-enough sufficient? For customer service chatbots, GPT-4's superior reasoning might justify the cost. For internal document classification, an open-source model might suffice.

Second, data sensitivity. Are you processing confidential information, personal health data, or proprietary business intelligence? If so, sending data to third-party APIs creates unacceptable risks. Open-source models you host internally provide better data governance.

Third, cost structure. API-based models charge per token, making costs variable and potentially unpredictable at scale. If you're processing millions of documents, costs add up quickly. Self-hosted models have upfront infrastructure costs but predictable ongoing expenses.

Fourth, customization needs. Do you need to fine-tune on domain-specific data? Proprietary APIs often don't allow fine-tuning, or charge premium prices for it. Open-source models give complete flexibility.

Fifth, latency requirements. If you need real-time responses with minimal latency, hosting models locally or near your users might be necessary. API calls to distant servers introduce network latency.

Let me share a case study: Netflix's use of transfer learning for recommendations. Netflix has hundreds of millions of users watching thousands of titles. Traditional recommendation systems used collaborative filtering: users who liked these movies also liked those movies. But collaborative filtering struggles with new content that lacks viewing history, the cold start problem.

Netflix adopted transfer learning by pre-training models on auxiliary tasks. They trained models to predict genres, actors, directors, and plot summaries from video frames and audio. These models learned rich representations of content. When recommending new titles, even without viewing history, the model understands the content based on these learned representations and can match it to users with similar preferences. This dramatically improved recommendations for new content, increasing user engagement and reducing churn.

Now let's explore future trends that will define the next decade of AI. First, artificial general intelligence, or AGI. Current AI systems are narrow, excelling at specific tasks but unable to generalize broadly like humans. GPT-4 is remarkable at language tasks but can't drive a car or cook dinner. AlphaGo dominates Go but can't play chess without retraining.

AGI would be different: a system with human-level intelligence across all domains. It could learn any intellectual task a human can learn, transfer knowledge flexibly between domains, reason about novel situations, and improve through experience. The timeline for AGI is highly debated. Some researchers predict within decades; others believe it's centuries away or may not be possible. As Chief Data and AI Officers, you should monitor progress but plan based on current capabilities rather than speculative futures.

Second, autonomous agents. Current AI systems are primarily reactive, responding to inputs but not acting independently toward goals. The next frontier is agentic AI: systems that set goals, make plans, take actions, and adapt based on outcomes. Imagine an AI assistant that doesn't just answer questions but proactively manages your calendar, drafts emails, schedules meetings, and handles routine tasks without constant supervision.

AutoGPT and BabyAGI are early experiments in this direction. They use language models to create task lists, execute tasks, and iterate based on results. While still limited, they hint at a future where AI systems are more autonomous and proactive. This raises profound questions about control, alignment, and safety that we'll explore in future sessions.

Third, multimodal AI. Humans perceive the world through multiple senses: vision, hearing, touch, smell, taste. We integrate these seamlessly. Current AI systems are typically unimodal, processing either text or images or audio. Multimodal models process multiple modalities simultaneously, enabling richer understanding.

GPT-4's vision capabilities let it answer questions about images. CLIP connects vision and language. Future systems will integrate even more modalities, perhaps including robotics with touch and proprioception. This will enable AI to interact with the physical world more effectively, driving advances in robotics, autonomous vehicles, and augmented reality.

Fourth, AI for science. AI is becoming an essential tool for scientific discovery. AlphaFold's protein structure predictions are accelerating drug discovery and our understanding of biology. AI systems discover new materials with desired properties by searching vast chemical spaces. They optimize complex experiments, analyze massive datasets from telescopes and particle accelerators, and generate hypotheses for researchers to test.

The potential is enormous. Drug discovery typically takes over a decade and billions of dollars. AI could reduce this dramatically, identifying promising compounds and predicting their properties before expensive lab work. Climate models could become more accurate with AI improving predictions of weather, sea-level rise, and extreme events. Personalized medicine could become reality as AI analyzes individual genetic profiles to recommend tailored treatments.

Fifth, energy-efficient AI. Current AI models are energy-intensive. Training GPT-3 consumed approximately 1,300 megawatt-hours, equivalent to the annual energy use of 120 homes. As models scale, energy consumption grows. This is economically expensive and environmentally unsustainable.

Researchers are exploring neuromorphic computing, which mimics brain architecture more closely. The brain's neurons communicate through spikes, discrete events rather than continuous signals. Neuromorphic chips use spiking neural networks, dramatically reducing energy consumption. Intel's Loihi and IBM's TrueNorth are early examples. Widespread adoption could enable AI systems running on batteries, opening new applications.

Sixth, federated learning. Traditional machine learning centralizes data: collect it all in one place, train a model, deploy it. But data privacy, regulations like GDPR, and practical constraints often make centralization impossible. Medical data from different hospitals can't easily be shared due to privacy laws. Smartphone data is too large and sensitive to upload wholesale.

Federated learning trains models across decentralized data. Instead of moving data to the model, you move the model to the data. Each node, like a hospital or smartphone, trains locally and shares only model updates, not raw data. These updates aggregate into a global model. This preserves privacy while enabling learning from distributed data.

Google uses federated learning for keyboard suggestions on Android devices. Your typing patterns train a local model on your phone. Model updates, not your actual text, are shared with Google's servers and aggregated with millions of other users' updates. The global model improves while your privacy is preserved.

Seventh, explainable AI. As AI systems make increasingly consequential decisions, from loan approvals to medical diagnoses, the black-box nature of deep learning becomes problematic. Explainable AI, or XAI, aims to make model decisions interpretable and transparent.

Techniques like LIME, Local Interpretable Model-agnostic Explanations, approximate complex models locally with simpler, interpretable models. SHAP, SHapley Additive exPlanations, uses game theory to assign importance scores to features. Attention mechanisms in transformers show which parts of input the model focused on when making decisions.

However, there's a fundamental trade-off between accuracy and interpretability. Simple models like decision trees are interpretable but often less accurate. Deep neural networks are highly accurate but opaque. As Chief Data and AI Officers, you'll need to balance this trade-off based on application requirements and regulatory constraints.

Eighth, AI alignment and safety. As AI systems become more powerful, ensuring they behave as intended becomes critical. Alignment is the problem of making AI systems whose goals and behaviors align with human values and intentions. This is harder than it sounds because human values are complex, contextual, and sometimes contradictory.

Researchers distinguish between two types of alignment. Outer alignment ensures the objective we give the AI system captures what we actually want. If you reward a cleaning robot for removing dirt, it might learn to hide dirt rather than clean it. Inner alignment ensures the AI system robustly pursues the objective we gave it rather than proxy goals it discovered during training.

Concrete safety challenges include robustness, ensuring AI systems perform reliably even in unusual situations; transparency, understanding what AI systems are doing and why; and control, maintaining human oversight and the ability to intervene. These challenges intensify as systems become more autonomous and capable.

In conclusion, transfer learning has made AI practical and accessible, allowing organizations to leverage pre-trained models and adapt them with limited data and resources. The explosion of foundation models from GPT-4 to Claude to open-source alternatives gives you choices with different trade-offs in performance, cost, control, and privacy. Understanding these trade-offs is essential for effective AI strategy.

Looking ahead, trends like autonomous agents, multimodal AI, AI for science, energy-efficient AI, federated learning, explainable AI, and AI alignment will shape the landscape. Staying informed about these developments helps you anticipate opportunities and challenges.

For your assignment, identify three foundation model use cases for your organization. Consider what tasks could benefit from transfer learning, which foundation models are most appropriate, and what trade-offs you'd need to manage. In our next session, we'll shift from AI to data strategy, exploring how to build the data foundations that enable effective AI deployment.

Thank you, and please complete the quiz on transfer learning and foundation models to reinforce today's concepts.`
    }
};

// You would continue this pattern for all 12 sessions...
// For the purpose of this file, I'm showing the structure for sessions 1 and 2
// The complete implementation would include all 12 sessions with 30-minute scripts each
